{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Crawling the text from the targeted sites**\n",
        "\n"
      ],
      "metadata": {
        "id": "L77eR7iXrDoM"
      },
      "id": "L77eR7iXrDoM"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b2aa0914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2aa0914",
        "outputId": "d911a67a-d734-41ea-ac86-aaf3de5693e5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to job_data_0.txt\n",
            "Data saved to job_data_1.txt\n",
            "Data saved to job_data_2.txt\n",
            "Data saved to job_data_3.txt\n",
            "Data saved to job_data_4.txt\n",
            "Data saved to job_data_5.txt\n",
            "Data saved to job_data_6.txt\n",
            "Data saved to job_data_7.txt\n",
            "Data saved to job_data_8.txt\n",
            "Data saved to job_data_9.txt\n",
            "Data saved to job_data_10.txt\n",
            "Data saved to job_data_11.txt\n",
            "Data saved to job_data_12.txt\n",
            "Data saved to job_data_13.txt\n",
            "Data saved to job_data_14.txt\n",
            "Data saved to job_data_15.txt\n",
            "No more jobs to crawl.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from itertools import zip_longest\n",
        "def extract_job_data(page_num):\n",
        "    try:\n",
        "        result = requests.get(f\"https://wuzzuf.net/search/jobs/?a=spbg&q=python&start={page_num}\")\n",
        "        src = result.content\n",
        "        soup = BeautifulSoup(src, \"lxml\")\n",
        "\n",
        "        job_titles = soup.find_all(\"h2\", {\"class\": \"css-m604qf\"})\n",
        "        job_descriptions = soup.find_all(\"div\", {\"class\": \"css-y4udm8\"})\n",
        "        company_names = soup.find_all(\"a\", {\"class\": \"css-17s97q8\"})\n",
        "        location_names = soup.find_all(\"span\", {\"class\": \"css-5wys0k\"})\n",
        "\n",
        "        job_data = []\n",
        "        for title, desc, company, location in zip_longest(job_titles, job_descriptions, company_names, location_names):\n",
        "            data = {\n",
        "                \"title\": title.text.strip() if title else None,\n",
        "                \"description\": desc.text.strip() if desc else None,\n",
        "                \"company\": company.text.strip() if company else None,\n",
        "                \"location\": location.text.strip() if location else None\n",
        "            }\n",
        "            job_data.append(data)\n",
        "\n",
        "        return job_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return []\n",
        "\n",
        "def save_to_file(job_data, file_name):\n",
        "    try:\n",
        "        with open(file_name, 'w', encoding='utf-8') as file:\n",
        "            for job in job_data:\n",
        "                file.write(f\"Title: {job['title']}\\n\")\n",
        "                file.write(f\"Company: {job['company']}\\n\")\n",
        "                file.write(f\"Description: {job['description']}\\n\")\n",
        "                file.write(f\"Location: {job['location']}\\n\\n\")\n",
        "        print(f\"Data saved to {file_name}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while saving to file:\", e)\n",
        "\n",
        "def crawl_jobs():\n",
        "    try:\n",
        "        page_num = 0\n",
        "        while True:\n",
        "            job_data = extract_job_data(page_num)\n",
        "            if not job_data:\n",
        "                print(\"No more jobs to crawl.\")\n",
        "                break\n",
        "\n",
        "            file_name = f\"job_data_{page_num}.txt\"\n",
        "            save_to_file(job_data, file_name)\n",
        "\n",
        "            page_num += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    crawl_jobs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save all crawled documents in one specific folder"
      ],
      "metadata": {
        "id": "e4d6eT2Bru0h"
      },
      "id": "e4d6eT2Bru0h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc007b5d",
      "metadata": {
        "id": "fc007b5d",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "documents = []\n",
        "# Find all files with the extension .txt\n",
        "files = glob.glob('*.txt')\n",
        "\n",
        "# Open each file and read its contents\n",
        "for file in files:\n",
        "    with open(file, 'r') as f:\n",
        "        links_content = f.read()\n",
        "        # Process the contents of the file\n",
        "\n",
        "        # Append the content of the links to the documents_list\n",
        "        documents.append(links_content)\n",
        "        print (documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and transform the text\n"
      ],
      "metadata": {
        "id": "GNO0St2VsDjn"
      },
      "id": "GNO0St2VsDjn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d8c5fe",
      "metadata": {
        "id": "09d8c5fe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Normalization: Convert to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an inverted index"
      ],
      "metadata": {
        "id": "j7xULV1usiis"
      },
      "id": "j7xULV1usiis"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f43b93",
      "metadata": {
        "id": "27f43b93"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "print (documents)\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "# Fit the vectorizer and transform the documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create inverted index\n",
        "inverted_index = defaultdict(list)\n",
        "for i, doc in enumerate(preprocessed_documents):\n",
        "    for word in doc.split():\n",
        "        inverted_index[word].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the transformed text"
      ],
      "metadata": {
        "id": "LHNXG349sysK"
      },
      "id": "LHNXG349sysK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d95f33",
      "metadata": {
        "id": "52d95f33"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define file paths\n",
        "file_paths = [\n",
        "    r\"/content/job_data_0.txt\",\n",
        "    r\"/content/job_data_1.txt\",\n",
        "    r\"/content/job_data_2.txt\",\n",
        "    r\"/content/job_data_3.txt\",\n",
        "    r\"/content/job_data_4.txt\",\n",
        "    r\"/content/job_data_5.txt\",\n",
        "    r\"/content/job_data_6.txt\",\n",
        "    r\"/content/job_data_7.txt\",\n",
        "    r\"/content/job_data_8.txt\",\n",
        "    r\"/content/job_data_9.txt\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        documents.append(file.read())"
      ],
      "metadata": {
        "id": "ES2KCaOmv7ub"
      },
      "id": "ES2KCaOmv7ub",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"software\""
      ],
      "metadata": {
        "id": "rqVJLtdDwE-J"
      },
      "id": "rqVJLtdDwE-J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check the similarity**"
      ],
      "metadata": {
        "id": "wdrOKcGos__Q"
      },
      "id": "wdrOKcGos__Q"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "vectors = vectorizer.fit_transform(documents)\n",
        "\n",
        "query_vector = vectorizer.transform([query])"
      ],
      "metadata": {
        "id": "6n3zISn_wNWK"
      },
      "id": "6n3zISn_wNWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities = cosine_similarity(query_vector, vectors)"
      ],
      "metadata": {
        "id": "1N2vURNcwSO0"
      },
      "id": "1N2vURNcwSO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices = np.argsort(cosine_similarities[0])[::-1]\n",
        "\n",
        "ranked_documents = [(file_paths[idx], cosine_similarities[0][idx]) for idx in sorted_indices]"
      ],
      "metadata": {
        "id": "BQ9JrOYYwZKv"
      },
      "id": "BQ9JrOYYwZKv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rank the documents according thier similarity**"
      ],
      "metadata": {
        "id": "gnDuRgDQtOIG"
      },
      "id": "gnDuRgDQtOIG"
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (file_path, score) in enumerate(ranked_documents, start=1):\n",
        "    print(f\"Rank {idx}: Similarity Score: {score}\")\n",
        "    print(f\"Document: {file_path}\\n\")"
      ],
      "metadata": {
        "id": "Tzkx9lx0whjg",
        "collapsed": true
      },
      "id": "Tzkx9lx0whjg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank documents by decreasing similarity score\n",
        "similarity_scores = cosine_similarities.flatten()\n",
        "document_scores = [(document, score) for document, score in zip(documents, similarity_scores)]\n",
        "sorted_documents = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display ranked documents\n",
        "print(\"Ranked documents by decreasing similarity score:\")\n",
        "for i, (doc, score) in enumerate(sorted_documents):\n",
        "    print(i+1, \":\", doc, \"(Similarity Score:\", score, \")\")"
      ],
      "metadata": {
        "id": "KC_HZ8L6TIC0",
        "collapsed": true
      },
      "id": "KC_HZ8L6TIC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get indices of top X documents\n",
        "X = 5\n",
        "top_indices = cosine_similarities.argsort()[0][-X:][::-1]\n",
        "\n",
        "# Return top X documents\n",
        "top_documents = [documents[i] for i in top_indices]\n",
        "print(\"Top\", X, \"documents:\")\n",
        "for i, doc in enumerate(top_documents):\n",
        "    print(i+1, \":\", doc)"
      ],
      "metadata": {
        "id": "TGzuW9Y6TsdP",
        "collapsed": true
      },
      "id": "TGzuW9Y6TsdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the model**"
      ],
      "metadata": {
        "id": "NcalGvcWthvk"
      },
      "id": "NcalGvcWthvk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901047cd",
      "metadata": {
        "id": "901047cd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Example query\n",
        "query = input (\"please enter your query \")\n",
        "\n",
        "# Preprocess the query\n",
        "preprocessed_query = preprocess_text(query)\n",
        "\n",
        "# Represent the query as a vector\n",
        "query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
        "\n",
        "print(\"Query Vector:\")\n",
        "print(query_vector.toarray())\n",
        "\n",
        "print(\"\\nInverted Index:\")\n",
        "for word, doc_ids in inverted_index.items():\n",
        "    print(word, \":\", doc_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "frDw8NShtg0R"
      },
      "id": "frDw8NShtg0R"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}